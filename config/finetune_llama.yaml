trainer:
  stop_early: False
  max_epochs: 200
  devices: 1
  strategy: 'deepspeed_stage_3'
  accelerator: 'gpu'
  fp: 16
  gas: 8

data:
  data_class: 'WenzhongQADataModel'
  data_class_module: "training.data_loader"
  data_path: '/root/autodl-tmp/dataset'
  max_seq_length: 512
  train_batchsize: 2
  valid_batchsize: 2
  do_eval_only: False
  n_ctx: 1024
  stride: 678
  num_workers: 16
  data_type_name: 'JSON'

tokenizer:
  #tokenizer_path: 'IDEA-CCNL/Wenzhong-GPT2-110M'
  tokenizer_path: 'IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese'

llama:
  #pretrained_file: "IDEA-CCNL/Wenzhong-GPT2-110M" 
  base_model: "openlmlab/open-chinese-llama-7b-patch" 
  cache_dir: '/root/autodl-tmp/llama_openlmlab'
  load_checkpoint: False
  lr: 0.0001
  weight_decay: 0.1
  learning_rate: 0.0001
  warmup: 0.01
lora:
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules: [
    "q_proj",
    "v_proj",
]

ckpt:
  monitor: 'train_loss'
  save_top_k: 1
  mode: 'min'
  every_n_train_steps: 100
  save_weights_only: True
  dirpath: '/root/autodl-tmp/ckpt'
  save_last: True
  file_name: 'Default'

test:
  ckpt_path: '/root/autodl-tmp/ckpt/test_0.01.pt'

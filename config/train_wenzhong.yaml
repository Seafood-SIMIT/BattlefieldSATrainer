trainer:
  stop_early: False
  chkpt_every_n_epochs: 20
  max_epochs: 100
  devices: 7
  strategy: 'deepspeed_stage_2'
  accelerator: 'gpu'

data:
  data_class: 'WenzhongQADataModel'
  data_class_module: "training.data_loader"
  data_path: 'files/dataset/wyl'
  data_type_name: 'csv'
  max_seq_length: 128
  train_batchsize: 2
  valid_batchsize: 2
  do_eval_only: False
  n_ctx: 1024
  stride: 678
  batch_size: 4
  num_workers: 16
  raw: True

tokenizer:
  #tokenizer_path: 'IDEA-CCNL/Wenzhong-GPT2-110M'
  tokenizer_path: 'IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese'
kg:
  model_name: 'bert-base-chinese'
  pretrained_file: 'files/kg_bert_final_model'
  num_labels: 3
  load_checkpoint: 'None'
  optimizer: 'Adam'
  lr: 1e-3
  loss: cross_entropy

gpt2:
  #pretrained_file: "IDEA-CCNL/Wenzhong-GPT2-110M" 
  pretrained_file: "IDEA-CCNL/Wenzhong2.0-GPT2-3.5B-chinese" 
  load_checkpoint: False
  loss: cross_entropy
  optimizer: 'Adam'
  lr: 0.001
  weight_decay: 0.1
  learning_rate: 0.0001
  warmup: 0.01
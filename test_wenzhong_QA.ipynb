{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import HParam\n",
    "hp = HParam('config/train_wenzhong.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 1\n",
      "CKPT Loading : /Users/sunlin/Documents/workdir/situationAwareness_0528/kg_trainer/files/rename.ckpt/final.ckpt\n"
     ]
    }
   ],
   "source": [
    "from gpt2_generator import gpt2_model_gpt2_generator, GPT2_BaseLitModel, WenzhongQALitModel\n",
    "from training.util import import_class, setup_data_from_args\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(hp.gpt2.pretrained_file)\n",
    "gpt2_model.load_state_dict(torch.load(hp.test.ckpt_path),False)\n",
    "    #data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(hp.gpt2.pretrained_file)\n",
    "\n",
    "gpt2_litmodel = WenzhongQALitModel(args=hp.gpt2, model=gpt2_model,num_data=1)\n",
    "print('CKPT Loading : '+hp.test.ckpt_path)\n",
    "#state_dict = torch.load(hp.test.ckpt_path,map_location = torch.device('cpu'))\n",
    "#state_dict = WenzhongQALitModel.load_from_checkpoint(hp.test.ckpt_path,args=hp.gpt2, model=gpt2_model,num_data=1)\n",
    "\n",
    "#gpt2_litmodel = gpt2_litmodel(args=hp.gpt2, model=gpt2_model,num_data=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question='27分;多云天气;敌方直升机出现在D7区域;我方坦克被敌方摧毁。'\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})\n",
    "inputs_dict = tokenizer.encode_plus(question,\n",
    "                                                 max_length=20, padding='max_length',\n",
    "                                                 truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = gpt2_model.generate(\n",
    "            input_ids=inputs_dict['input_ids'],\n",
    "            attention_mask=inputs_dict['attention_mask'],\n",
    "            max_length=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=1.0,\n",
    "            repetition_penalty=2.5,\n",
    "            # early_stopping=True,\n",
    "            num_return_sequences=5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27分;多云天气;敌方直升机出现在D7区域;我方坦克被敌方摧毁。\n",
      "27分;多云天气;敌方直升机；穿甲弹及警戒动能下降。无牵帮、不雷试射咖野战1星/2飞衣舰体内郁闫的一条圈套公会额座安装吸壶遥浪者3D841G图(4×17)去平“龂泱之武神-净剑士寐士迤逃 火琯很菙�然抱怨 悬念已所件�〘�斂�·妭�将 冒歬�光惩�濽畏魔王6.04 4,700 -711%902KMB$13332095匲�彩俑姖压例 邪人三栫�五使久�5AFFV1428806712346618293637383940\n",
      "\n",
      "=================================\n",
      "\n",
      "27分;多云天气;敌方直升机桥墩；符合任动一阶段性成就(1-4)苏斯特兄弟中C世代的所地包国农权县。这是1912干时3.949⎞法王义大帝几位众人之闭口不诳寒衫老者 呷床剪当很辅 帜愤惑、姚鬼金士武使甄士隐·昆�官�——俫�省逎�雅�狶�竂�武—作〈� 悪神 答 黒魔浲� 追琁�将  � 上奉皊�龙忒務�孚醒舐三郗遗光子誒胆栖 天�楞裀視�… 朶食畯田 —————————————————————\n",
      "\n",
      "=================================\n",
      "\n",
      "27分;多云天气;敌方直升机移开后降落在埋伏的佩架之东（仅一台就胸郁闷得衣飘浮冲进入无人匹俦者中厢、条斯九难…… 版节通高36.25mm 快打B 法电遭雪女凤般惶惖 〘�评妒上帿�泶�役 花奈�巭�庡戗孂�呎�眩�楞 鬼忲�魔劚醒愆�竂�幙�焁�\n",
      "\n",
      "=================================\n",
      "\n",
      "27分;多云天气;敌方直升机可能导走/点射。操作步骤ﾋ埃野武装战斗飞衣圈中门询妙法、䖅八力避开减逐郭吉宁大主今早一般2.8-4左右的远隧章冈趸佷雒三九·6M+1D′PWKZH5J\n",
      "\n",
      "=================================\n",
      "\n",
      "27分;多云天气;敌方直升机绕圈进衅 符合规定高飞库渡险以及A！C全都是布优走位的 后勤对截几义功量 中层想覶残快退之难 板田并不断弯遗 跖子波浜栒竣囲写�胂�三船说奶�愲�巩�抱扶歂�俆�杈与剑士作耂�琁�大導�。‛\n",
      "\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#question = tokenizer.decode(inputs_dict['input_ids'])\n",
    "print(question)\n",
    "for sample in generated_ids:\n",
    "        preds = [tokenizer.decode(sample, skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True)]\n",
    "        preds = ''.join(preds)\n",
    "        print(preds)\n",
    "        print('\\n=================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gpt2_litmodel.generate(inputs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39;49mdecode(result\u001b[39m.\u001b[39;49mlogits)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mps/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3476\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3474\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3476\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3477\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3478\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3479\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3480\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3481\u001b[0m )\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mps/lib/python3.9/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    928\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    933\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mps/lib/python3.9/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer.decode(result.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_outputs = gpt2_litmodel.predict(tokenizer('我方装甲车编队已到达E8区域，准备进行反击。',return_tensors='pt'))\n",
    "\n",
    "for idx, sentense in enumerate(generation_outputs.sequences):\n",
    "    print('next sentence %d:\\n'%idx,\n",
    "    tokenizer.decode(sentense).split('<|endoftext|>')[0])\n",
    "    print('*'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

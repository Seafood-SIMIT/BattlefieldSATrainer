{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import HParam\n",
    "hp = HParam('config/test_wenzhong.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT2 Model\n",
      "num_data: 1\n"
     ]
    }
   ],
   "source": [
    "from gpt2_generator import gpt2_model_gpt2_generator, GPT2_BaseLitModel, WenzhongQALitModel\n",
    "from training.util import import_class, setup_data_from_args\n",
    "from transformers import GPT2Tokenizer\n",
    "gpt2_model = gpt2_model_gpt2_generator(hp.gpt2.pretrained_file)\n",
    "    #data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(hp.tokenizer.tokenizer_path)\n",
    "\n",
    "gpt2_litmodel = WenzhongQALitModel\n",
    "#gpt2_litmodel = gpt2_litmodel.load_from_checkpoint(hp.gpt2.chkpt_path, args=hp.gpt2, model=gpt2_model,num_data=1)\n",
    "gpt2_litmodel = gpt2_litmodel(args=hp.gpt2, model=gpt2_model,num_data=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "csv处理进度: 2it [00:00, 667.14it/s]\n",
      "csv处理进度: 1it [00:00, 2041.02it/s]\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer.predict` requires `forward` method to run.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data, tokenizer\u001b[39m=\u001b[39m setup_data_from_args(hp)\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Trainer\u001b[39m.\u001b[39mfrom_argparse_args(hp\u001b[39m.\u001b[39mtrainer)\n\u001b[0;32m----> 6\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m      7\u001b[0m             gpt2_litmodel, data, ckpt_path\u001b[39m=\u001b[39;49mhp\u001b[39m.\u001b[39;49mgpt2\u001b[39m.\u001b[39;49mchkpt_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:892\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    890\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    891\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 892\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    893\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    894\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:938\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    933\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    934\u001b[0m )\n\u001b[1;32m    936\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predicted_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    940\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    941\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1038\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_callbacks()\n\u001b[1;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector\u001b[39m.\u001b[39m_attach_model_logging_functions()\n\u001b[0;32m-> 1038\u001b[0m verify_loop_configurations(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1040\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: preparing data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:49\u001b[0m, in \u001b[0;36mverify_loop_configurations\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     __verify_eval_loop_configuration(model, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[39melif\u001b[39;00m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mPREDICTING:\n\u001b[0;32m---> 49\u001b[0m     __verify_eval_loop_configuration(model, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     51\u001b[0m __verify_batch_transfer_support(trainer)\n\u001b[1;32m     52\u001b[0m \u001b[39m# TODO: Delete this check in v2.0\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nv/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:125\u001b[0m, in \u001b[0;36m__verify_eval_loop_configuration\u001b[0;34m(model, stage)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39m`predict_step` cannot be None to run `Trainer.predict`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_step \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_overridden(\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m, model):\n\u001b[0;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39m`Trainer.predict` requires `forward` method to run.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39m# -----------------------------------\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m# verify model has an eval_step\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39m# -----------------------------------\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m has_step:\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer.predict` requires `forward` method to run."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, loggers\n",
    "\n",
    "data, tokenizer= setup_data_from_args(hp)\n",
    "\n",
    "trainer = Trainer.from_argparse_args(hp.trainer)\n",
    "result = trainer.predict(\n",
    "            gpt2_litmodel, data, ckpt_path=hp.gpt2.chkpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next sentence 0:\n",
      " 我方装甲车编队已到达E8区域，准备进行反击。” [1] 词条图册 更多国家 解读词语 盲点漫画 一个美团打败滴拉后的故事 传奇档次的超越之旅 第八期 满天星 世界首富大逃杀 抢劫一枚支票 惊天秘密的海底下盗墓者 红河谷中的探险者 中回族网络版的大肆屠村 数万中外游客�\n",
      "****************************************\n",
      "next sentence 1:\n",
      " 我方装甲车编队已到达E8区域，准备进行反击。面对严密的包围圈﹐沿E7北侧向西扩张～展开。E9医院在一个多月的时间内就组筑了起来￼﻿。主力被出动的足以毁坏ﺧ﷦ﾝ ？\n",
      "****************************************\n",
      "next sentence 2:\n",
      " 我方装甲车编队已到达E8区域，准备进行反击。E9北部包括了美国和俄罗斯～后来战场上的最大成员￥21.00元。该赛事将于9月25日在加州纽瓦克医院举街开跑﹐有望圆满截�334公里的完结。 参考规则 1. 本次联谊会设立展位ﺦ400个。 2. 计时地点为比利时每英金额1.1百万�\n",
      "****************************************\n",
      "next sentence 3:\n",
      " 我方装甲车编队已到达E8区域，准备进行反击。在一番激战后～英军首先 password 发射了红色的弹道导引炮﹐观察瞄出事态发展的趋势﻿获得二百多枚的情报￥双方合力向西攻凿的方案ﺿ。第一颗火箭被掳去﷽短暂的消息由自己的组群及装头提供ﾞ。最吸张的是\n",
      "****************************************\n",
      "next sentence 4:\n",
      " 我方装甲车编队已到达E8区域，准备进行反击。战斗打响后﹐E9营的各种运动员齐聚E10北方高地～E11第四纵陆空领馆￥16ﾐ﻿16 符号�憎ﳬﶓﰯﬓ七.超现实派技术 超宇宙火箭炮和美国石油巨头的对比憐ﷳﺴ﯒照ﴏ普林斯顿研究报告穀僵�\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "generation_outputs = gpt2_litmodel.predict(tokenizer('我方装甲车编队已到达E8区域，准备进行反击。',return_tensors='pt'))\n",
    "\n",
    "for idx, sentense in enumerate(generation_outputs.sequences):\n",
    "    print('next sentence %d:\\n'%idx,\n",
    "    tokenizer.decode(sentense).split('<|endoftext|>')[0])\n",
    "    print('*'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
